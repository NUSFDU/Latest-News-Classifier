{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = os.path.dirname(cwd) # .../Latest-News-Classifier/0. Latest News Classifier\n",
    "\n",
    "# Dataframe\n",
    "path_df = parent + \"/03. Feature Engineering/Pickles/df.pickle\"\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "\n",
    "# features_train\n",
    "path_features_train = parent + \"/03. Feature Engineering/Pickles/features_train.pickle\"\n",
    "with open(path_features_train, 'rb') as data:\n",
    "    features_train = pickle.load(data)\n",
    "\n",
    "# labels_train\n",
    "path_labels_train = parent + \"/03. Feature Engineering/Pickles/labels_train.pickle\"\n",
    "with open(path_labels_train, 'rb') as data:\n",
    "    labels_train = pickle.load(data)\n",
    "\n",
    "# features_test\n",
    "path_features_test = parent + \"/03. Feature Engineering/Pickles/features_test.pickle\"\n",
    "with open(path_features_test, 'rb') as data:\n",
    "    features_test = pickle.load(data)\n",
    "\n",
    "# labels_test\n",
    "path_labels_test = parent + \"/03. Feature Engineering/Pickles/labels_test.pickle\"\n",
    "with open(path_labels_test, 'rb') as data:\n",
    "    labels_test = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dimension of our feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1891, 300)\n",
      "(334, 300)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can see what hyperparameters the model has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'base_score': None,\n",
      " 'booster': None,\n",
      " 'colsample_bylevel': None,\n",
      " 'colsample_bynode': None,\n",
      " 'colsample_bytree': None,\n",
      " 'gamma': None,\n",
      " 'gpu_id': None,\n",
      " 'importance_type': 'gain',\n",
      " 'interaction_constraints': None,\n",
      " 'learning_rate': None,\n",
      " 'max_delta_step': None,\n",
      " 'max_depth': None,\n",
      " 'min_child_weight': None,\n",
      " 'missing': nan,\n",
      " 'monotone_constraints': None,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'num_parallel_tree': None,\n",
      " 'objective': 'binary:logistic',\n",
      " 'random_state': 8,\n",
      " 'reg_alpha': None,\n",
      " 'reg_lambda': None,\n",
      " 'scale_pos_weight': None,\n",
      " 'subsample': None,\n",
      " 'tree_method': None,\n",
      " 'validate_parameters': None,\n",
      " 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "xgb_0 = xgb.XGBClassifier(random_state=8)\n",
    "\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(xgb_0.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll tune the following ones:\n",
    "\n",
    "Tree-related hyperparameters:\n",
    "* `n_estimators` = number of trees in the forest.\n",
    "* `max_depth` = max number of levels in each decision tree\n",
    "* `max_delta_step` = max delta step we allow each leaf output to be. If it is set to a positive value, it can help making the update step more conservative\n",
    "\n",
    "Boosting-related hyperparameters:\n",
    "* `learning_rate`= learning rate shrinks the contribution of each tree by learning_rate.\n",
    "* `subsample`= the fraction of samples to be used for fitting the individual base learners.\n",
    "\n",
    "XGBoost [API Reference](https://xgboost.readthedocs.io/en/latest/python/python_intro.html#data-interface), [Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define the grid. Since we have a huge amount of hyperparameters, we'll try few values for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.1, 0.5],\n",
      " 'max_delta_step': [1, 5, 10, None],\n",
      " 'max_depth': [10, 20, 40, None],\n",
      " 'n_estimators': [200, 500, 800],\n",
      " 'subsample': [0.5, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "n_estimators = [200, 500, 800]\n",
    "\n",
    "# max_depth\n",
    "max_depth = [10, 20, 40]\n",
    "max_depth.append(None)\n",
    "\n",
    "# max_delta_step\n",
    "max_delta_step = [1, 5, 10, None]\n",
    "\n",
    "# learning rate\n",
    "learning_rate = [.1, .5]\n",
    "\n",
    "# subsample\n",
    "subsample = [.5, 1.]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'max_delta_step': max_delta_step,\n",
    "               'learning_rate': learning_rate,\n",
    "               'subsample': subsample}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll perform the Random Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 40.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "       min_child_w..._pos_weight=None, subsample=None,\n",
       "       tree_method=None, validate_parameters=None, verbosity=None),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [200, 500, 800], 'max_depth': [10, 20, 40, None], 'max_delta_step': [1, 5, 10, None], 'learning_rate': [0.1, 0.5], 'subsample': [0.5, 1.0]},\n",
       "          pre_dispatch='2*n_jobs', random_state=8, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "xgbc = xgb.XGBClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=xgbc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best hyperparameters resulting from the Random Search:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'subsample': 0.5, 'n_estimators': 200, 'max_depth': 40, 'max_delta_step': 1, 'learning_rate': 0.1}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.951348492861\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can do a more exhaustive search centered in those values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=8, test_size=0.33, train_size=None),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "       min_child_w..._pos_weight=None, subsample=None,\n",
       "       tree_method=None, validate_parameters=None, verbosity=None),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [20, 40, 50], 'max_delta_step': [1.0], 'n_estimators': [200, 500], 'learning_rate': [0.1, 0.5], 'subsample': [0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "max_depth = [20, 40, 50]\n",
    "n_estimators = [200, 500]\n",
    "max_delta_step = [1.0]\n",
    "learning_rate = [.1, .5]\n",
    "subsample = [.5]\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': max_depth,\n",
    "    'max_delta_step': max_delta_step,\n",
    "    'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample\n",
    "\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "xgbc = xgb.XGBClassifier(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=xgbc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters turn out to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Grid Search are:\n",
      "{'learning_rate': 0.1, 'max_delta_step': 1.0, 'max_depth': 20, 'n_estimators': 200, 'subsample': 0.5}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.950933333333\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Grid Search are:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model in `best_xgbc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgbc = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.1, max_delta_step=1.0, max_depth=20,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='multi:softprob', random_state=8, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "       tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best gradient boosting model. Let's fit it and see how it performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit the model to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.1, max_delta_step=1.0, max_depth=20,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
       "       objective='multi:softprob', random_state=8, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "       tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgbc.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_pred = best_xgbc.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional class probabilities can be obtained by typing:\n",
    "\n",
    "`xgbc_pred = best_xgbc.predict_proba(features_test)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance analysis, we will use the confusion matrix, the classification report and the accuracy on both training and test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_xgbc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.934131736527\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, xgbc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91        81\n",
      "           1       0.96      0.96      0.96        49\n",
      "           2       0.95      0.85      0.90        72\n",
      "           3       0.99      0.97      0.98        72\n",
      "           4       0.92      0.95      0.93        60\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       334\n",
      "   macro avg       0.94      0.94      0.94       334\n",
      "weighted avg       0.94      0.93      0.93       334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,xgbc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_df = df[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, xgbc_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['Category'].values, \n",
    "            yticklabels=aux_df['Category'].values,\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could get the average time the model takes to get predictions. We want the algorithm to be fast since we are creating an app which will gather data from the internet and get the predicted categories. However, since the difference when predicting 10-20 observations will be very little, we won't take this into account.\n",
    "\n",
    "However, the code below could do this task:\n",
    "\n",
    "```python\n",
    "features_time = features_train\n",
    "elapsed_list = []\n",
    "for i in range(0,10):\n",
    "    \n",
    "    start = time.time()\n",
    "    predictions = best_lrc.predict(features_time)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    elapsed_list.append(elapsed)\n",
    "\n",
    "mean_time_elapsed = np.mean(elapsed_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the hyperparameter tuning process has returned a better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92814371257485029"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = xgb.XGBClassifier(random_state = 8)\n",
    "base_model.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, base_model.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93413173652694614"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgbc.fit(features_train, labels_train)\n",
    "accuracy_score(labels_test, best_xgbc.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dataset with a model summary to compare models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "     'Model': 'XGBoost',\n",
    "     'Training Set Accuracy': accuracy_score(labels_train, best_xgbc.predict(features_train)),\n",
    "     'Test Set Accuracy': accuracy_score(labels_test, xgbc_pred)\n",
    "}\n",
    "\n",
    "df_models_xgbc = pd.DataFrame(d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.934132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Training Set Accuracy  Test Set Accuracy\n",
       "0  XGBoost                    1.0           0.934132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models_xgbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model and this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/best_xgbc.pickle', 'wb') as output:\n",
    "    pickle.dump(best_xgbc, output)\n",
    "    \n",
    "with open('Models/df_models_xgbc.pickle', 'wb') as output:\n",
    "    pickle.dump(df_models_xgbc, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the top features based on the impurity-based feature importances.\n",
    "\n",
    "The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technology' 'user' 'labour' 'growth' 'minister' 'film' 'match' 'market'\n",
      " 'tory' 'online' 'star' 'computer' 'award' 'party' 'win' 'music' 'analyst'\n",
      " 'game' 'year old' 'bank' 'player' 'company' 'pc' 'government' 'digital'\n",
      " 'election' 'economy' 'club' 'firm' 'economic' 'video' 'mr' 'tv' 'software'\n",
      " 'eu' 'mobile' 'test' 'minute' 'mail' 'china' 'head' 'lord' 'ireland'\n",
      " 'price' 'title' 'program' 'fall' 'concern' 'use' 'release']\n"
     ]
    }
   ],
   "source": [
    "# labels_test\n",
    "path_tfidf = parent + \"/03. Feature Engineering/Pickles/tfidf.pickle\"\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)\n",
    "\n",
    "\n",
    "indices = np.argsort(best_xgbc.feature_importances_)[-50:]\n",
    "feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "print(feature_names[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ticks = np.arange(0, len(feature_names))\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(y_ticks, best_xgbc.feature_importances_[indices])\n",
    "ax.set_yticklabels(feature_names)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"XGBoost Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
