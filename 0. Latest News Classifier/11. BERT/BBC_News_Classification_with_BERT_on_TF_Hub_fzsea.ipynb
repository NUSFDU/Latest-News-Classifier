{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJNG6m0RsUTz"
   },
   "source": [
    "Reference: [BERT Github](https://github.com/google-research/bert); [Text Classification Tutorial](https://www.tensorflow.org/hub/tutorials/tf2_text_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# BBC News Classification with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "Part of the BBC News Classification [project](https://github.com/fzsea/Latest-News-Classifier/tree/nusfdu). Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "hhbGEfwgdEtw",
    "outputId": "89e6d90e-481f-4f68-b314-acedefbc9456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import run_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "b31dbcee-4e67-4c82-cef3-3dab2e0ba1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: OUTPUT_DIR_NAME *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted on the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_CzeeATVuPK"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "DATA_PATH = 'https://github.com/fzsea/Latest-News-Classifier/raw/nusfdu/0.%20Latest%20News%20Classifier/01.%20Dataset%20Creation/News_dataset.csv'\n",
    "urllib.request.urlretrieve(DATA_PATH, 'News_dataset.csv')\n",
    "\n",
    "raw_data = pd.read_csv('/content/News_dataset.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Ju1LB63HaJx6",
    "outputId": "259ad47d-db64-4a5c-b0f2-333c00b08f9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "      <td>001.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "      <td>002.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "      <td>003.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "      <td>004.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "      <td>005.txt-business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name  ... Complete_Filename\n",
       "0   001.txt  ...  001.txt-business\n",
       "1   002.txt  ...  002.txt-business\n",
       "2   003.txt  ...  003.txt-business\n",
       "3   004.txt  ...  004.txt-business\n",
       "4   005.txt  ...  005.txt-business\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtL_PtEFYoap"
   },
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "\n",
    "raw_data['Category_Code'] = raw_data['Category']\n",
    "raw_data = raw_data.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoB84gmjYqsO"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(raw_data,\n",
    "                               test_size=0.15,\n",
    "                               random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "fa7aba6c-74de-4506-8aaf-a1aca9749731"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['File_Name', 'Content', 'Category', 'Complete_Filename',\n",
       "       'Category_Code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Gx5gh2CuXN_y",
    "outputId": "82df1c77-59bd-426c-c924-5a3d167ce18b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "      <th>Category_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>058.txt</td>\n",
       "      <td>BT boosts its broadband packages\\n\\nBritish Te...</td>\n",
       "      <td>tech</td>\n",
       "      <td>058.txt-tech</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>209.txt</td>\n",
       "      <td>Abbas 'will not tolerate' attacks\\n\\nPalestini...</td>\n",
       "      <td>politics</td>\n",
       "      <td>209.txt-politics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>359.txt</td>\n",
       "      <td>GM in crunch talks on Fiat future\\n\\nFiat will...</td>\n",
       "      <td>business</td>\n",
       "      <td>359.txt-business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>479.txt</td>\n",
       "      <td>Ford gains from finance not cars\\n\\nFord, the ...</td>\n",
       "      <td>business</td>\n",
       "      <td>479.txt-business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>168.txt</td>\n",
       "      <td>MPs' murder sentence concern\\n\\nMurder sentenc...</td>\n",
       "      <td>politics</td>\n",
       "      <td>168.txt-politics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_Name  ... Category_Code\n",
       "1881   058.txt  ...             4\n",
       "1104   209.txt  ...             2\n",
       "358    359.txt  ...             0\n",
       "478    479.txt  ...             0\n",
       "1063   168.txt  ...             2\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'Content'\n",
    "LABEL_COLUMN = 'Category_Code'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XLT236lKgClv",
    "outputId": "d150b755-abf7-48a2-d322-675499d0e9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_InputExamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qhsks48bYcSN",
    "outputId": "e2d0c454-56b3-438e-843c-0ea526949673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.indexing._LocIndexer at 0x7f8726c9c3b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples.loc(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "5TmeMToFgMgp",
    "outputId": "0a99a42c-a0ea-4412-c4e9-b9407935079f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1881    <bert.run_classifier.InputExample object at 0x...\n",
       "1104    <bert.run_classifier.InputExample object at 0x...\n",
       "358     <bert.run_classifier.InputExample object at 0x...\n",
       "478     <bert.run_classifier.InputExample object at 0x...\n",
       "1063    <bert.run_classifier.InputExample object at 0x...\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "ca905b54-c7d8-4a74-bfa2-61599f1c4f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "dsBo6RCtQmwx",
    "outputId": "8717a0ff-06dc-48b7-a0be-f8051850a4cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "00e577fc-9dd3-4174-85a3-a626a39965be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bt boost ##s its broadband packages british telecom has said it will double the broadband speeds of most of its home and business customers . the increased speeds will come at no extra charge and follows a similar move by internet service provider ao ##l . many bt customers will now have download speeds of 2 ##mb ##ps , although there are usage allowance ##s of between one gig ##aby ##te and 30 gig ##aby ##tes a month . the new speeds start to come into effect on 17 february for home customers and 1 april for businesses . \" britain is now broadband britain , \" said duncan ingram , bt ' s managing director , broadband and internet services . he added : \" ninety percent of our customers will see real increases in speed . \" these speed increases will give people the opportunity to do a lot more with their broadband connections , \" he said . up ##load speeds - the speed at which information is sent from a pc via broadband - will remain at the same speed , said mr ingram . despite the increases , bt will continue to have usage allowance ##s for home customers . \" the allowance ##s are extremely generous , \" said mr ingram \" for what we are seeing in the market place - they are really not an issue . \" bt will begin enforcing the allowance ##s in the summer . customers who exceed the amounts will either [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bt boost ##s its broadband packages british telecom has said it will double the broadband speeds of most of its home and business customers . the increased speeds will come at no extra charge and follows a similar move by internet service provider ao ##l . many bt customers will now have download speeds of 2 ##mb ##ps , although there are usage allowance ##s of between one gig ##aby ##te and 30 gig ##aby ##tes a month . the new speeds start to come into effect on 17 february for home customers and 1 april for businesses . \" britain is now broadband britain , \" said duncan ingram , bt ' s managing director , broadband and internet services . he added : \" ninety percent of our customers will see real increases in speed . \" these speed increases will give people the opportunity to do a lot more with their broadband connections , \" he said . up ##load speeds - the speed at which information is sent from a pc via broadband - will remain at the same speed , said mr ingram . despite the increases , bt will continue to have usage allowance ##s for home customers . \" the allowance ##s are extremely generous , \" said mr ingram \" for what we are seeing in the market place - they are really not an issue . \" bt will begin enforcing the allowance ##s in the summer . customers who exceed the amounts will either [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 18411 12992 2015 2049 19595 14555 2329 18126 2038 2056 2009 2097 3313 1996 19595 10898 1997 2087 1997 2049 2188 1998 2449 6304 1012 1996 3445 10898 2097 2272 2012 2053 4469 3715 1998 4076 1037 2714 2693 2011 4274 2326 10802 20118 2140 1012 2116 18411 6304 2097 2085 2031 8816 10898 1997 1016 14905 4523 1010 2348 2045 2024 8192 21447 2015 1997 2090 2028 15453 21275 2618 1998 2382 15453 21275 4570 1037 3204 1012 1996 2047 10898 2707 2000 2272 2046 3466 2006 2459 2337 2005 2188 6304 1998 1015 2258 2005 5661 1012 1000 3725 2003 2085 19595 3725 1010 1000 2056 7343 23231 1010 18411 1005 1055 6605 2472 1010 19595 1998 4274 2578 1012 2002 2794 1024 1000 13568 3867 1997 2256 6304 2097 2156 2613 7457 1999 3177 1012 1000 2122 3177 7457 2097 2507 2111 1996 4495 2000 2079 1037 2843 2062 2007 2037 19595 7264 1010 1000 2002 2056 1012 2039 11066 10898 1011 1996 3177 2012 2029 2592 2003 2741 2013 1037 7473 3081 19595 1011 2097 3961 2012 1996 2168 3177 1010 2056 2720 23231 1012 2750 1996 7457 1010 18411 2097 3613 2000 2031 8192 21447 2015 2005 2188 6304 1012 1000 1996 21447 2015 2024 5186 12382 1010 1000 2056 2720 23231 1000 2005 2054 2057 2024 3773 1999 1996 3006 2173 1011 2027 2024 2428 2025 2019 3277 1012 1000 18411 2097 4088 27455 1996 21447 2015 1999 1996 2621 1012 6304 2040 13467 1996 8310 2097 2593 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 18411 12992 2015 2049 19595 14555 2329 18126 2038 2056 2009 2097 3313 1996 19595 10898 1997 2087 1997 2049 2188 1998 2449 6304 1012 1996 3445 10898 2097 2272 2012 2053 4469 3715 1998 4076 1037 2714 2693 2011 4274 2326 10802 20118 2140 1012 2116 18411 6304 2097 2085 2031 8816 10898 1997 1016 14905 4523 1010 2348 2045 2024 8192 21447 2015 1997 2090 2028 15453 21275 2618 1998 2382 15453 21275 4570 1037 3204 1012 1996 2047 10898 2707 2000 2272 2046 3466 2006 2459 2337 2005 2188 6304 1998 1015 2258 2005 5661 1012 1000 3725 2003 2085 19595 3725 1010 1000 2056 7343 23231 1010 18411 1005 1055 6605 2472 1010 19595 1998 4274 2578 1012 2002 2794 1024 1000 13568 3867 1997 2256 6304 2097 2156 2613 7457 1999 3177 1012 1000 2122 3177 7457 2097 2507 2111 1996 4495 2000 2079 1037 2843 2062 2007 2037 19595 7264 1010 1000 2002 2056 1012 2039 11066 10898 1011 1996 3177 2012 2029 2592 2003 2741 2013 1037 7473 3081 19595 1011 2097 3961 2012 1996 2168 3177 1010 2056 2720 23231 1012 2750 1996 7457 1010 18411 2097 3613 2000 2031 8192 21447 2015 2005 2188 6304 1012 1000 1996 21447 2015 2024 5186 12382 1010 1000 2056 2720 23231 1000 2005 2054 2057 2024 3773 1999 1996 3006 2173 1011 2027 2024 2428 2025 2019 3277 1012 1000 18411 2097 4088 27455 1996 21447 2015 1999 1996 2621 1012 6304 2040 13467 1996 8310 2097 2593 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 4 (id = 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] abbas ' will not tolerate ' attacks palestinian leader mahmoud abbas has said he will not tolerate attacks such as last friday ' s suicide bombing in the israeli city of tel aviv . in an interview ahead of a meeting in london to discuss palestinian reforms , mr abbas said such attacks were against palestinian interests . the palestinian authority ( pa ) was ex ##ert ##ing \" a 100 % effort \" to end the violence , mr abbas added . the attack , which killed five , was the first of its kind since he took office . mr abbas confirmed israel shared information with the pa in the hunt for the organise ##rs of the attack . the israeli government refuses to accept syria ' s denial ##s that it was implicated in the nightclub bombing . israeli officials gave an intelligence briefing to foreign ambassadors on monday , explaining syria ' s alleged involvement . british foreign minister jack straw said there had been a \" continuing stream \" of information suggesting palestinian militant groups were operating from within syria . in an email interview in the british newspaper the independent , mr abbas said : \" we believe peace is possible now and we are ready to negotiate with israel to reach a true and lasting peace based on justice and international legitimacy . \" he added : \" we have an opportunity and it would be ir ##res ##pon ##sible if we , the israelis , or [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] abbas ' will not tolerate ' attacks palestinian leader mahmoud abbas has said he will not tolerate attacks such as last friday ' s suicide bombing in the israeli city of tel aviv . in an interview ahead of a meeting in london to discuss palestinian reforms , mr abbas said such attacks were against palestinian interests . the palestinian authority ( pa ) was ex ##ert ##ing \" a 100 % effort \" to end the violence , mr abbas added . the attack , which killed five , was the first of its kind since he took office . mr abbas confirmed israel shared information with the pa in the hunt for the organise ##rs of the attack . the israeli government refuses to accept syria ' s denial ##s that it was implicated in the nightclub bombing . israeli officials gave an intelligence briefing to foreign ambassadors on monday , explaining syria ' s alleged involvement . british foreign minister jack straw said there had been a \" continuing stream \" of information suggesting palestinian militant groups were operating from within syria . in an email interview in the british newspaper the independent , mr abbas said : \" we believe peace is possible now and we are ready to negotiate with israel to reach a true and lasting peace based on justice and international legitimacy . \" he added : \" we have an opportunity and it would be ir ##res ##pon ##sible if we , the israelis , or [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 17532 1005 2097 2025 19242 1005 4491 9302 3003 27278 17532 2038 2056 2002 2097 2025 19242 4491 2107 2004 2197 5958 1005 1055 5920 8647 1999 1996 5611 2103 1997 10093 12724 1012 1999 2019 4357 3805 1997 1037 3116 1999 2414 2000 6848 9302 8818 1010 2720 17532 2056 2107 4491 2020 2114 9302 5426 1012 1996 9302 3691 1006 6643 1007 2001 4654 8743 2075 1000 1037 2531 1003 3947 1000 2000 2203 1996 4808 1010 2720 17532 2794 1012 1996 2886 1010 2029 2730 2274 1010 2001 1996 2034 1997 2049 2785 2144 2002 2165 2436 1012 2720 17532 4484 3956 4207 2592 2007 1996 6643 1999 1996 5690 2005 1996 22933 2869 1997 1996 2886 1012 1996 5611 2231 10220 2000 5138 7795 1005 1055 14920 2015 2008 2009 2001 20467 1999 1996 15479 8647 1012 5611 4584 2435 2019 4454 27918 2000 3097 20986 2006 6928 1010 9990 7795 1005 1055 6884 6624 1012 2329 3097 2704 2990 13137 2056 2045 2018 2042 1037 1000 5719 5460 1000 1997 2592 9104 9302 16830 2967 2020 4082 2013 2306 7795 1012 1999 2019 10373 4357 1999 1996 2329 3780 1996 2981 1010 2720 17532 2056 1024 1000 2057 2903 3521 2003 2825 2085 1998 2057 2024 3201 2000 13676 2007 3956 2000 3362 1037 2995 1998 9879 3521 2241 2006 3425 1998 2248 22568 1012 1000 2002 2794 1024 1000 2057 2031 2019 4495 1998 2009 2052 2022 20868 6072 26029 19307 2065 2057 1010 1996 28363 1010 2030 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 17532 1005 2097 2025 19242 1005 4491 9302 3003 27278 17532 2038 2056 2002 2097 2025 19242 4491 2107 2004 2197 5958 1005 1055 5920 8647 1999 1996 5611 2103 1997 10093 12724 1012 1999 2019 4357 3805 1997 1037 3116 1999 2414 2000 6848 9302 8818 1010 2720 17532 2056 2107 4491 2020 2114 9302 5426 1012 1996 9302 3691 1006 6643 1007 2001 4654 8743 2075 1000 1037 2531 1003 3947 1000 2000 2203 1996 4808 1010 2720 17532 2794 1012 1996 2886 1010 2029 2730 2274 1010 2001 1996 2034 1997 2049 2785 2144 2002 2165 2436 1012 2720 17532 4484 3956 4207 2592 2007 1996 6643 1999 1996 5690 2005 1996 22933 2869 1997 1996 2886 1012 1996 5611 2231 10220 2000 5138 7795 1005 1055 14920 2015 2008 2009 2001 20467 1999 1996 15479 8647 1012 5611 4584 2435 2019 4454 27918 2000 3097 20986 2006 6928 1010 9990 7795 1005 1055 6884 6624 1012 2329 3097 2704 2990 13137 2056 2045 2018 2042 1037 1000 5719 5460 1000 1997 2592 9104 9302 16830 2967 2020 4082 2013 2306 7795 1012 1999 2019 10373 4357 1999 1996 2329 3780 1996 2981 1010 2720 17532 2056 1024 1000 2057 2903 3521 2003 2825 2085 1998 2057 2024 3201 2000 13676 2007 3956 2000 3362 1037 2995 1998 9879 3521 2241 2006 3425 1998 2248 22568 1012 1000 2002 2794 1024 1000 2057 2031 2019 4495 1998 2009 2052 2022 20868 6072 26029 19307 2065 2057 1010 1996 28363 1010 2030 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] gm in crunch talks on fiat future fiat will meet car giant general motors ( gm ) on tuesday in an attempt to reach agreement over the future of the italian firm ' s loss - making auto group . fiat claims that gm is legally obliged to buy the 90 % of the car unit it does not already own ; gm says the contract , signed in 2000 , is no longer valid . press reports have speculated that fiat may be willing to accept a cash payment in return for dropping its claim . both companies want to cut costs as the car industry adjust ##s to wan ##ing demand . the meeting between fiat boss sergio march ##ion ##ne and gm ' s rick wagon ##er is due to take place at 133 ##0 gm ##t in zurich , according to the reuters news agency . mr march ##ion ##ne is confident of his firm ' s legal position , saying in an interview with the financial times that gm ' s argument \" has no legs \" . the agreement in question dates back to gm ' s decision to buy 20 % of fiat ' s auto division in 2000 . at the time , it gave the italian firm the right , via a ' put option ' , to sell the remaining stake to gm . in recent weeks , fiat has reiterated its claims that this ' put ' is still valid and legally binding [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] gm in crunch talks on fiat future fiat will meet car giant general motors ( gm ) on tuesday in an attempt to reach agreement over the future of the italian firm ' s loss - making auto group . fiat claims that gm is legally obliged to buy the 90 % of the car unit it does not already own ; gm says the contract , signed in 2000 , is no longer valid . press reports have speculated that fiat may be willing to accept a cash payment in return for dropping its claim . both companies want to cut costs as the car industry adjust ##s to wan ##ing demand . the meeting between fiat boss sergio march ##ion ##ne and gm ' s rick wagon ##er is due to take place at 133 ##0 gm ##t in zurich , according to the reuters news agency . mr march ##ion ##ne is confident of his firm ' s legal position , saying in an interview with the financial times that gm ' s argument \" has no legs \" . the agreement in question dates back to gm ' s decision to buy 20 % of fiat ' s auto division in 2000 . at the time , it gave the italian firm the right , via a ' put option ' , to sell the remaining stake to gm . in recent weeks , fiat has reiterated its claims that this ' put ' is still valid and legally binding [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 13938 1999 24514 7566 2006 18550 2925 18550 2097 3113 2482 5016 2236 9693 1006 13938 1007 2006 9857 1999 2019 3535 2000 3362 3820 2058 1996 2925 1997 1996 3059 3813 1005 1055 3279 1011 2437 8285 2177 1012 18550 4447 2008 13938 2003 10142 14723 2000 4965 1996 3938 1003 1997 1996 2482 3131 2009 2515 2025 2525 2219 1025 13938 2758 1996 3206 1010 2772 1999 2456 1010 2003 2053 2936 9398 1012 2811 4311 2031 15520 2008 18550 2089 2022 5627 2000 5138 1037 5356 7909 1999 2709 2005 7510 2049 4366 1012 2119 3316 2215 2000 3013 5366 2004 1996 2482 3068 14171 2015 2000 14071 2075 5157 1012 1996 3116 2090 18550 5795 13983 2233 3258 2638 1998 13938 1005 1055 6174 9540 2121 2003 2349 2000 2202 2173 2012 14506 2692 13938 2102 1999 10204 1010 2429 2000 1996 26665 2739 4034 1012 2720 2233 3258 2638 2003 9657 1997 2010 3813 1005 1055 3423 2597 1010 3038 1999 2019 4357 2007 1996 3361 2335 2008 13938 1005 1055 6685 1000 2038 2053 3456 1000 1012 1996 3820 1999 3160 5246 2067 2000 13938 1005 1055 3247 2000 4965 2322 1003 1997 18550 1005 1055 8285 2407 1999 2456 1012 2012 1996 2051 1010 2009 2435 1996 3059 3813 1996 2157 1010 3081 1037 1005 2404 5724 1005 1010 2000 5271 1996 3588 8406 2000 13938 1012 1999 3522 3134 1010 18550 2038 28960 2049 4447 2008 2023 1005 2404 1005 2003 2145 9398 1998 10142 8031 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 13938 1999 24514 7566 2006 18550 2925 18550 2097 3113 2482 5016 2236 9693 1006 13938 1007 2006 9857 1999 2019 3535 2000 3362 3820 2058 1996 2925 1997 1996 3059 3813 1005 1055 3279 1011 2437 8285 2177 1012 18550 4447 2008 13938 2003 10142 14723 2000 4965 1996 3938 1003 1997 1996 2482 3131 2009 2515 2025 2525 2219 1025 13938 2758 1996 3206 1010 2772 1999 2456 1010 2003 2053 2936 9398 1012 2811 4311 2031 15520 2008 18550 2089 2022 5627 2000 5138 1037 5356 7909 1999 2709 2005 7510 2049 4366 1012 2119 3316 2215 2000 3013 5366 2004 1996 2482 3068 14171 2015 2000 14071 2075 5157 1012 1996 3116 2090 18550 5795 13983 2233 3258 2638 1998 13938 1005 1055 6174 9540 2121 2003 2349 2000 2202 2173 2012 14506 2692 13938 2102 1999 10204 1010 2429 2000 1996 26665 2739 4034 1012 2720 2233 3258 2638 2003 9657 1997 2010 3813 1005 1055 3423 2597 1010 3038 1999 2019 4357 2007 1996 3361 2335 2008 13938 1005 1055 6685 1000 2038 2053 3456 1000 1012 1996 3820 1999 3160 5246 2067 2000 13938 1005 1055 3247 2000 4965 2322 1003 1997 18550 1005 1055 8285 2407 1999 2456 1012 2012 1996 2051 1010 2009 2435 1996 3059 3813 1996 2157 1010 3081 1037 1005 2404 5724 1005 1010 2000 5271 1996 3588 8406 2000 13938 1012 1999 3522 3134 1010 18550 2038 28960 2049 4447 2008 2023 1005 2404 1005 2003 2145 9398 1998 10142 8031 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ford gains from finance not cars ford , the us car company , reported higher fourth quarter and full - year profits on thursday boosted by a bu ##oya ##nt period for its car loans unit . net income for 2004 was $ 3 . 5 ##bn ( a ## ##1 . 87 ##bn ) - up nearly $ 3 ##bn from 2003 - while turnover rose $ 7 . 2 ##bn to $ 170 . 8 ##bn . in the fourth quarter alone ford reported net income of $ 104 ##m , compared with a loss of $ 79 ##3 ##m a year ago . but its auto unit made a loss . fourth quarter turnover was $ 44 . 7 ##bn , compared to $ 45 . 9 ##bn a year ago . though car and truck loan profits saved the day , ford ' s auto unit made a pre - tax loss of $ 470 ##m in the fourth quarter ( compared to a profit of a ## ##13 ##m in the year - ago period ) and its us sales dipped 3 . 8 % . yesterday general motor ' s results also showed its finance unit was a strong contributor to profits . however , ford is working hard to rev ##ital ##ise its product portfolio , un ##ve ##iling the fusion and ze ##phy ##r models at the international motor show in detroit . it also brought out a number of new models in the second half [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ford gains from finance not cars ford , the us car company , reported higher fourth quarter and full - year profits on thursday boosted by a bu ##oya ##nt period for its car loans unit . net income for 2004 was $ 3 . 5 ##bn ( a ## ##1 . 87 ##bn ) - up nearly $ 3 ##bn from 2003 - while turnover rose $ 7 . 2 ##bn to $ 170 . 8 ##bn . in the fourth quarter alone ford reported net income of $ 104 ##m , compared with a loss of $ 79 ##3 ##m a year ago . but its auto unit made a loss . fourth quarter turnover was $ 44 . 7 ##bn , compared to $ 45 . 9 ##bn a year ago . though car and truck loan profits saved the day , ford ' s auto unit made a pre - tax loss of $ 470 ##m in the fourth quarter ( compared to a profit of a ## ##13 ##m in the year - ago period ) and its us sales dipped 3 . 8 % . yesterday general motor ' s results also showed its finance unit was a strong contributor to profits . however , ford is working hard to rev ##ital ##ise its product portfolio , un ##ve ##iling the fusion and ze ##phy ##r models at the international motor show in detroit . it also brought out a number of new models in the second half [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4811 12154 2013 5446 2025 3765 4811 1010 1996 2149 2482 2194 1010 2988 3020 2959 4284 1998 2440 1011 2095 11372 2006 9432 28043 2011 1037 20934 18232 3372 2558 2005 2049 2482 10940 3131 1012 5658 3318 2005 2432 2001 1002 1017 1012 1019 24700 1006 1037 29646 2487 1012 6584 24700 1007 1011 2039 3053 1002 1017 24700 2013 2494 1011 2096 20991 3123 1002 1021 1012 1016 24700 2000 1002 10894 1012 1022 24700 1012 1999 1996 2959 4284 2894 4811 2988 5658 3318 1997 1002 9645 2213 1010 4102 2007 1037 3279 1997 1002 6535 2509 2213 1037 2095 3283 1012 2021 2049 8285 3131 2081 1037 3279 1012 2959 4284 20991 2001 1002 4008 1012 1021 24700 1010 4102 2000 1002 3429 1012 1023 24700 1037 2095 3283 1012 2295 2482 1998 4744 5414 11372 5552 1996 2154 1010 4811 1005 1055 8285 3131 2081 1037 3653 1011 4171 3279 1997 1002 21064 2213 1999 1996 2959 4284 1006 4102 2000 1037 5618 1997 1037 29646 17134 2213 1999 1996 2095 1011 3283 2558 1007 1998 2049 2149 4341 13537 1017 1012 1022 1003 1012 7483 2236 5013 1005 1055 3463 2036 3662 2049 5446 3131 2001 1037 2844 12130 2000 11372 1012 2174 1010 4811 2003 2551 2524 2000 7065 18400 5562 2049 4031 11103 1010 4895 3726 16281 1996 10077 1998 27838 21281 2099 4275 2012 1996 2248 5013 2265 1999 5626 1012 2009 2036 2716 2041 1037 2193 1997 2047 4275 1999 1996 2117 2431 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4811 12154 2013 5446 2025 3765 4811 1010 1996 2149 2482 2194 1010 2988 3020 2959 4284 1998 2440 1011 2095 11372 2006 9432 28043 2011 1037 20934 18232 3372 2558 2005 2049 2482 10940 3131 1012 5658 3318 2005 2432 2001 1002 1017 1012 1019 24700 1006 1037 29646 2487 1012 6584 24700 1007 1011 2039 3053 1002 1017 24700 2013 2494 1011 2096 20991 3123 1002 1021 1012 1016 24700 2000 1002 10894 1012 1022 24700 1012 1999 1996 2959 4284 2894 4811 2988 5658 3318 1997 1002 9645 2213 1010 4102 2007 1037 3279 1997 1002 6535 2509 2213 1037 2095 3283 1012 2021 2049 8285 3131 2081 1037 3279 1012 2959 4284 20991 2001 1002 4008 1012 1021 24700 1010 4102 2000 1002 3429 1012 1023 24700 1037 2095 3283 1012 2295 2482 1998 4744 5414 11372 5552 1996 2154 1010 4811 1005 1055 8285 3131 2081 1037 3653 1011 4171 3279 1997 1002 21064 2213 1999 1996 2959 4284 1006 4102 2000 1037 5618 1997 1037 29646 17134 2213 1999 1996 2095 1011 3283 2558 1007 1998 2049 2149 4341 13537 1017 1012 1022 1003 1012 7483 2236 5013 1005 1055 3463 2036 3662 2049 5446 3131 2001 1037 2844 12130 2000 11372 1012 2174 1010 4811 2003 2551 2524 2000 7065 18400 5562 2049 4031 11103 1010 4895 3726 16281 1996 10077 1998 27838 21281 2099 4275 2012 1996 2248 5013 2265 1999 5626 1012 2009 2036 2716 2041 1037 2193 1997 2047 4275 1999 1996 2117 2431 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] mps ' murder sentence concern murder sentences should not be reduced automatically simply because of a guilty plea , says a new mps ' report . the influential commons home affairs committee was responding to sentencing guidelines issued this summer . the mps also call for tough ##er sentences for crimes committed under the influence of drink or drugs . they say the influence of drugs and alcohol should be introduced as an ag ##gra ##vating factor when judges and magistrates sentence offenders . committee chairman john den ##ham said drugs of alcohol were sometimes used as an excuse . \" the committee believes that these arguments should be rejected by sentence ##rs and that being under their influence should instead be an ag ##gra ##vating factor . \" at present judges , when sentencing murderers to the mandatory life sentence , can reduce the tariff - the minimum term they must serve - if the defendant plead ##s guilty . but although they are spared the ordeal of a trial many murder victims ' relatives are unhappy . in july this year amanda champion ' s killer , james ford , pleaded guilty to her murder and was jailed for at least 15 years - it would have been longer had he denied the charge . amanda ' s uncle , lewis champion , told the bbc news website ford did not deserve any credit for his plea , saying : \" nothing at all is worth taking five years off a murder [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] mps ' murder sentence concern murder sentences should not be reduced automatically simply because of a guilty plea , says a new mps ' report . the influential commons home affairs committee was responding to sentencing guidelines issued this summer . the mps also call for tough ##er sentences for crimes committed under the influence of drink or drugs . they say the influence of drugs and alcohol should be introduced as an ag ##gra ##vating factor when judges and magistrates sentence offenders . committee chairman john den ##ham said drugs of alcohol were sometimes used as an excuse . \" the committee believes that these arguments should be rejected by sentence ##rs and that being under their influence should instead be an ag ##gra ##vating factor . \" at present judges , when sentencing murderers to the mandatory life sentence , can reduce the tariff - the minimum term they must serve - if the defendant plead ##s guilty . but although they are spared the ordeal of a trial many murder victims ' relatives are unhappy . in july this year amanda champion ' s killer , james ford , pleaded guilty to her murder and was jailed for at least 15 years - it would have been longer had he denied the charge . amanda ' s uncle , lewis champion , told the bbc news website ford did not deserve any credit for his plea , saying : \" nothing at all is worth taking five years off a murder [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 12616 1005 4028 6251 5142 4028 11746 2323 2025 2022 4359 8073 3432 2138 1997 1037 5905 14865 1010 2758 1037 2047 12616 1005 3189 1012 1996 6383 7674 2188 3821 2837 2001 14120 2000 23280 11594 3843 2023 2621 1012 1996 12616 2036 2655 2005 7823 2121 11746 2005 6997 5462 2104 1996 3747 1997 4392 2030 5850 1012 2027 2360 1996 3747 1997 5850 1998 6544 2323 2022 3107 2004 2019 12943 17643 26477 5387 2043 6794 1998 23007 6251 19591 1012 2837 3472 2198 7939 3511 2056 5850 1997 6544 2020 2823 2109 2004 2019 8016 1012 1000 1996 2837 7164 2008 2122 9918 2323 2022 5837 2011 6251 2869 1998 2008 2108 2104 2037 3747 2323 2612 2022 2019 12943 17643 26477 5387 1012 1000 2012 2556 6794 1010 2043 23280 28882 2000 1996 10915 2166 6251 1010 2064 5547 1996 23234 1011 1996 6263 2744 2027 2442 3710 1011 2065 1996 13474 25803 2015 5905 1012 2021 2348 2027 2024 16891 1996 23304 1997 1037 3979 2116 4028 5694 1005 9064 2024 12511 1012 1999 2251 2023 2095 8282 3410 1005 1055 6359 1010 2508 4811 1010 12254 5905 2000 2014 4028 1998 2001 21278 2005 2012 2560 2321 2086 1011 2009 2052 2031 2042 2936 2018 2002 6380 1996 3715 1012 8282 1005 1055 4470 1010 4572 3410 1010 2409 1996 4035 2739 4037 4811 2106 2025 10107 2151 4923 2005 2010 14865 1010 3038 1024 1000 2498 2012 2035 2003 4276 2635 2274 2086 2125 1037 4028 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 12616 1005 4028 6251 5142 4028 11746 2323 2025 2022 4359 8073 3432 2138 1997 1037 5905 14865 1010 2758 1037 2047 12616 1005 3189 1012 1996 6383 7674 2188 3821 2837 2001 14120 2000 23280 11594 3843 2023 2621 1012 1996 12616 2036 2655 2005 7823 2121 11746 2005 6997 5462 2104 1996 3747 1997 4392 2030 5850 1012 2027 2360 1996 3747 1997 5850 1998 6544 2323 2022 3107 2004 2019 12943 17643 26477 5387 2043 6794 1998 23007 6251 19591 1012 2837 3472 2198 7939 3511 2056 5850 1997 6544 2020 2823 2109 2004 2019 8016 1012 1000 1996 2837 7164 2008 2122 9918 2323 2022 5837 2011 6251 2869 1998 2008 2108 2104 2037 3747 2323 2612 2022 2019 12943 17643 26477 5387 1012 1000 2012 2556 6794 1010 2043 23280 28882 2000 1996 10915 2166 6251 1010 2064 5547 1996 23234 1011 1996 6263 2744 2027 2442 3710 1011 2065 1996 13474 25803 2015 5905 1012 2021 2348 2027 2024 16891 1996 23304 1997 1037 3979 2116 4028 5694 1005 9064 2024 12511 1012 1999 2251 2023 2095 8282 3410 1005 1055 6359 1010 2508 4811 1010 12254 5905 2000 2014 4028 1998 2001 21278 2005 2012 2560 2321 2086 1011 2009 2052 2031 2042 2936 2018 2002 6380 1996 3715 1012 8282 1005 1055 4470 1010 4572 3410 1010 2409 1996 4035 2739 4037 4811 2106 2025 10107 2151 4923 2005 2010 14865 1010 3038 1024 1000 2498 2012 2035 2003 4276 2635 2274 2086 2125 1037 4028 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ireland call up un ##cap ##ped campbell ulster sc ##rum - half ki ##eran campbell is one of five un ##cap ##ped players included in ireland ' s rb ##s six nations squad . campbell is joined by ulster colleagues roger wilson and ronan mcc ##or ##mack along with connacht ' s bernard jack ##man and munster ' s shaun payne . gordon d ' arc ##y is back after injury while munster flank ##er alan quinlan also returns to international consideration . \" the squad is selected purely on form . a lot of players put their hands up , \" coach eddie o ' sullivan told bbc sport . \" ki ##eran campbell was just one of those players . he has been playing very well in the he ##ine ##ken cup and deserves his call - up . \" there is big competition in some departments and not so much in others . there were one or two players who were unfortunate just to miss out . \" back - row forwards david wallace and victor costello are omitted , with o ' sullivan having quinlan , wilson , simon easter ##by , anthony foley , denis lea ##my and johnny o ' connor v ##ying for the three positions . with david humphrey ##s , kevin mag ##gs , simon best and tommy bow ##e again included , it is ulster ' s biggest representation in a training panel for quite some time . munster and leinster have 12 and [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] ireland call up un ##cap ##ped campbell ulster sc ##rum - half ki ##eran campbell is one of five un ##cap ##ped players included in ireland ' s rb ##s six nations squad . campbell is joined by ulster colleagues roger wilson and ronan mcc ##or ##mack along with connacht ' s bernard jack ##man and munster ' s shaun payne . gordon d ' arc ##y is back after injury while munster flank ##er alan quinlan also returns to international consideration . \" the squad is selected purely on form . a lot of players put their hands up , \" coach eddie o ' sullivan told bbc sport . \" ki ##eran campbell was just one of those players . he has been playing very well in the he ##ine ##ken cup and deserves his call - up . \" there is big competition in some departments and not so much in others . there were one or two players who were unfortunate just to miss out . \" back - row forwards david wallace and victor costello are omitted , with o ' sullivan having quinlan , wilson , simon easter ##by , anthony foley , denis lea ##my and johnny o ' connor v ##ying for the three positions . with david humphrey ##s , kevin mag ##gs , simon best and tommy bow ##e again included , it is ulster ' s biggest representation in a training panel for quite some time . munster and leinster have 12 and [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3163 2655 2039 4895 17695 5669 6063 11059 8040 6824 1011 2431 11382 23169 6063 2003 2028 1997 2274 4895 17695 5669 2867 2443 1999 3163 1005 1055 21144 2015 2416 3741 4686 1012 6063 2003 2587 2011 11059 8628 5074 4267 1998 18633 23680 2953 26945 2247 2007 27062 1005 1055 6795 2990 2386 1998 11348 1005 1055 16845 13470 1012 5146 1040 1005 8115 2100 2003 2067 2044 4544 2096 11348 12205 2121 5070 28451 2036 5651 2000 2248 9584 1012 1000 1996 4686 2003 3479 11850 2006 2433 1012 1037 2843 1997 2867 2404 2037 2398 2039 1010 1000 2873 5752 1051 1005 7624 2409 4035 4368 1012 1000 11382 23169 6063 2001 2074 2028 1997 2216 2867 1012 2002 2038 2042 2652 2200 2092 1999 1996 2002 3170 7520 2452 1998 17210 2010 2655 1011 2039 1012 1000 2045 2003 2502 2971 1999 2070 7640 1998 2025 2061 2172 1999 2500 1012 2045 2020 2028 2030 2048 2867 2040 2020 15140 2074 2000 3335 2041 1012 1000 2067 1011 5216 19390 2585 7825 1998 5125 21015 2024 16647 1010 2007 1051 1005 7624 2383 28451 1010 4267 1010 4079 10957 3762 1010 4938 17106 1010 11064 12203 8029 1998 5206 1051 1005 6720 1058 14147 2005 1996 2093 4460 1012 2007 2585 15462 2015 1010 4901 23848 5620 1010 4079 2190 1998 6838 6812 2063 2153 2443 1010 2009 2003 11059 1005 1055 5221 6630 1999 1037 2731 5997 2005 3243 2070 2051 1012 11348 1998 15684 2031 2260 1998 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3163 2655 2039 4895 17695 5669 6063 11059 8040 6824 1011 2431 11382 23169 6063 2003 2028 1997 2274 4895 17695 5669 2867 2443 1999 3163 1005 1055 21144 2015 2416 3741 4686 1012 6063 2003 2587 2011 11059 8628 5074 4267 1998 18633 23680 2953 26945 2247 2007 27062 1005 1055 6795 2990 2386 1998 11348 1005 1055 16845 13470 1012 5146 1040 1005 8115 2100 2003 2067 2044 4544 2096 11348 12205 2121 5070 28451 2036 5651 2000 2248 9584 1012 1000 1996 4686 2003 3479 11850 2006 2433 1012 1037 2843 1997 2867 2404 2037 2398 2039 1010 1000 2873 5752 1051 1005 7624 2409 4035 4368 1012 1000 11382 23169 6063 2001 2074 2028 1997 2216 2867 1012 2002 2038 2042 2652 2200 2092 1999 1996 2002 3170 7520 2452 1998 17210 2010 2655 1011 2039 1012 1000 2045 2003 2502 2971 1999 2070 7640 1998 2025 2061 2172 1999 2500 1012 2045 2020 2028 2030 2048 2867 2040 2020 15140 2074 2000 3335 2041 1012 1000 2067 1011 5216 19390 2585 7825 1998 5125 21015 2024 16647 1010 2007 1051 1005 7624 2383 28451 1010 4267 1010 4079 10957 3762 1010 4938 17106 1010 11064 12203 8029 1998 5206 1051 1005 6720 1058 14147 2005 1996 2093 4460 1012 2007 2585 15462 2015 1010 4901 23848 5620 1010 4079 2190 1998 6838 6812 2063 2153 2443 1010 2009 2003 11059 1005 1055 5221 6630 1999 1037 2731 5997 2005 3243 2070 2051 1012 11348 1998 15684 2031 2260 1998 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] gu ##rk ##has to help tsunami victims britain has offered to send a company of 120 gu ##rk ##has to assist with the tsunami relief effort in indonesia , downing street said . the deployment would involve troops from the 2nd battalion royal gu ##rk ##ha rifles , based in brunei . discussions have begun with indonesia on the exact timing and location of the deployment , but the government said the offer was aimed at the ace ##h province . downing st said a similar offer might be made to the sri lankan government . however a spokesman pointed out that there were particular logistical difficulties in indonesia which the gu ##rk ##has might be able to help with . the spokesman said : \" following this morning ' s daily coordination meeting on the post - tsunami relief effort , the government has formally offered the indonesian government the assistance of a company of british army gu ##rk ##has from 2nd battalion royal gu ##rk ##ha rifles around 120 personnel and two helicopters . \" this is in addition to the ships and aircraft we have already committed to the relief operation in the indian ocean . \" indonesia was by far the country worst affected by the tsunami , with 94 , 000 of the 140 , 000 confirmed deaths so far . international development minister gareth thomas said the assistance offer would most likely focus on the northern province of ace ##h . \" we have offered the gu ##rk [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] gu ##rk ##has to help tsunami victims britain has offered to send a company of 120 gu ##rk ##has to assist with the tsunami relief effort in indonesia , downing street said . the deployment would involve troops from the 2nd battalion royal gu ##rk ##ha rifles , based in brunei . discussions have begun with indonesia on the exact timing and location of the deployment , but the government said the offer was aimed at the ace ##h province . downing st said a similar offer might be made to the sri lankan government . however a spokesman pointed out that there were particular logistical difficulties in indonesia which the gu ##rk ##has might be able to help with . the spokesman said : \" following this morning ' s daily coordination meeting on the post - tsunami relief effort , the government has formally offered the indonesian government the assistance of a company of british army gu ##rk ##has from 2nd battalion royal gu ##rk ##ha rifles around 120 personnel and two helicopters . \" this is in addition to the ships and aircraft we have already committed to the relief operation in the indian ocean . \" indonesia was by far the country worst affected by the tsunami , with 94 , 000 of the 140 , 000 confirmed deaths so far . international development minister gareth thomas said the assistance offer would most likely focus on the northern province of ace ##h . \" we have offered the gu ##rk [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 19739 8024 14949 2000 2393 19267 5694 3725 2038 3253 2000 4604 1037 2194 1997 6036 19739 8024 14949 2000 6509 2007 1996 19267 4335 3947 1999 6239 1010 22501 2395 2056 1012 1996 10813 2052 9125 3629 2013 1996 3416 4123 2548 19739 8024 3270 9494 1010 2241 1999 18692 1012 10287 2031 5625 2007 6239 2006 1996 6635 10984 1998 3295 1997 1996 10813 1010 2021 1996 2231 2056 1996 3749 2001 6461 2012 1996 9078 2232 2874 1012 22501 2358 2056 1037 2714 3749 2453 2022 2081 2000 1996 5185 16159 2231 1012 2174 1037 14056 4197 2041 2008 2045 2020 3327 28961 8190 1999 6239 2029 1996 19739 8024 14949 2453 2022 2583 2000 2393 2007 1012 1996 14056 2056 1024 1000 2206 2023 2851 1005 1055 3679 12016 3116 2006 1996 2695 1011 19267 4335 3947 1010 1996 2231 2038 6246 3253 1996 9003 2231 1996 5375 1997 1037 2194 1997 2329 2390 19739 8024 14949 2013 3416 4123 2548 19739 8024 3270 9494 2105 6036 5073 1998 2048 12400 1012 1000 2023 2003 1999 2804 2000 1996 3719 1998 2948 2057 2031 2525 5462 2000 1996 4335 3169 1999 1996 2796 4153 1012 1000 6239 2001 2011 2521 1996 2406 5409 5360 2011 1996 19267 1010 2007 6365 1010 2199 1997 1996 8574 1010 2199 4484 6677 2061 2521 1012 2248 2458 2704 20243 2726 2056 1996 5375 3749 2052 2087 3497 3579 2006 1996 2642 2874 1997 9078 2232 1012 1000 2057 2031 3253 1996 19739 8024 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 19739 8024 14949 2000 2393 19267 5694 3725 2038 3253 2000 4604 1037 2194 1997 6036 19739 8024 14949 2000 6509 2007 1996 19267 4335 3947 1999 6239 1010 22501 2395 2056 1012 1996 10813 2052 9125 3629 2013 1996 3416 4123 2548 19739 8024 3270 9494 1010 2241 1999 18692 1012 10287 2031 5625 2007 6239 2006 1996 6635 10984 1998 3295 1997 1996 10813 1010 2021 1996 2231 2056 1996 3749 2001 6461 2012 1996 9078 2232 2874 1012 22501 2358 2056 1037 2714 3749 2453 2022 2081 2000 1996 5185 16159 2231 1012 2174 1037 14056 4197 2041 2008 2045 2020 3327 28961 8190 1999 6239 2029 1996 19739 8024 14949 2453 2022 2583 2000 2393 2007 1012 1996 14056 2056 1024 1000 2206 2023 2851 1005 1055 3679 12016 3116 2006 1996 2695 1011 19267 4335 3947 1010 1996 2231 2038 6246 3253 1996 9003 2231 1996 5375 1997 1037 2194 1997 2329 2390 19739 8024 14949 2013 3416 4123 2548 19739 8024 3270 9494 2105 6036 5073 1998 2048 12400 1012 1000 2023 2003 1999 2804 2000 1996 3719 1998 2948 2057 2031 2525 5462 2000 1996 4335 3169 1999 1996 2796 4153 1012 1000 6239 2001 2011 2521 1996 2406 5409 5360 2011 1996 19267 1010 2007 6365 1010 2199 1997 1996 8574 1010 2199 4484 6677 2061 2521 1012 2248 2458 2704 20243 2726 2056 1996 5375 3749 2052 2087 3497 3579 2006 1996 2642 2874 1997 9078 2232 1012 1000 2057 2031 3253 1996 19739 8024 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] egypt and israel seal trade deal in a sign of a tha ##w in relations between egypt and israel , the two countries have signed a trade protocol with the us , allowing egyptian goods made in partnership with israeli firms free access to american markets . the protocol , signed in cairo , will establish what are called \" qualified industrial zones \" in egypt . products from these zones will enjoy duty free access to the us , provided that 35 % of their components are the product of israeli - egyptian cooperation . the us describes this as the most important economic agreement between egypt and israel in two decades . the protocol establishing the zones has been stalled for years . there has been deep sensitivity in egypt about any form of co - operation with israel as long as its peace process with the palestinians remains blocked . but in recent weeks an unusual warmth has crept into relations between the two countries . both exchanged prisoners earlier this month , with egypt handing back an israeli who has served eight years in prison after being convicted for spying . egyptian president ho ##s ##ni mu ##bara ##k has described israeli prime minister ariel sharon as the best chance for the palestinians to achieve peace . the government in cairo now believes mr sharon is moving towards the centre and away from the positions of right wing groups . it also believes the us , pressed by europe , [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] egypt and israel seal trade deal in a sign of a tha ##w in relations between egypt and israel , the two countries have signed a trade protocol with the us , allowing egyptian goods made in partnership with israeli firms free access to american markets . the protocol , signed in cairo , will establish what are called \" qualified industrial zones \" in egypt . products from these zones will enjoy duty free access to the us , provided that 35 % of their components are the product of israeli - egyptian cooperation . the us describes this as the most important economic agreement between egypt and israel in two decades . the protocol establishing the zones has been stalled for years . there has been deep sensitivity in egypt about any form of co - operation with israel as long as its peace process with the palestinians remains blocked . but in recent weeks an unusual warmth has crept into relations between the two countries . both exchanged prisoners earlier this month , with egypt handing back an israeli who has served eight years in prison after being convicted for spying . egyptian president ho ##s ##ni mu ##bara ##k has described israeli prime minister ariel sharon as the best chance for the palestinians to achieve peace . the government in cairo now believes mr sharon is moving towards the centre and away from the positions of right wing groups . it also believes the us , pressed by europe , [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 5279 1998 3956 7744 3119 3066 1999 1037 3696 1997 1037 22794 2860 1999 4262 2090 5279 1998 3956 1010 1996 2048 3032 2031 2772 1037 3119 8778 2007 1996 2149 1010 4352 6811 5350 2081 1999 5386 2007 5611 9786 2489 3229 2000 2137 6089 1012 1996 8778 1010 2772 1999 11096 1010 2097 5323 2054 2024 2170 1000 4591 3919 10019 1000 1999 5279 1012 3688 2013 2122 10019 2097 5959 4611 2489 3229 2000 1996 2149 1010 3024 2008 3486 1003 1997 2037 6177 2024 1996 4031 1997 5611 1011 6811 6792 1012 1996 2149 5577 2023 2004 1996 2087 2590 3171 3820 2090 5279 1998 3956 1999 2048 5109 1012 1996 8778 7411 1996 10019 2038 2042 20659 2005 2086 1012 2045 2038 2042 2784 14639 1999 5279 2055 2151 2433 1997 2522 1011 3169 2007 3956 2004 2146 2004 2049 3521 2832 2007 1996 21524 3464 8534 1012 2021 1999 3522 3134 2019 5866 8251 2038 13147 2046 4262 2090 1996 2048 3032 1012 2119 10573 5895 3041 2023 3204 1010 2007 5279 13041 2067 2019 5611 2040 2038 2366 2809 2086 1999 3827 2044 2108 7979 2005 22624 1012 6811 2343 7570 2015 3490 14163 20709 2243 2038 2649 5611 3539 2704 16126 10666 2004 1996 2190 3382 2005 1996 21524 2000 6162 3521 1012 1996 2231 1999 11096 2085 7164 2720 10666 2003 3048 2875 1996 2803 1998 2185 2013 1996 4460 1997 2157 3358 2967 1012 2009 2036 7164 1996 2149 1010 4508 2011 2885 1010 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 5279 1998 3956 7744 3119 3066 1999 1037 3696 1997 1037 22794 2860 1999 4262 2090 5279 1998 3956 1010 1996 2048 3032 2031 2772 1037 3119 8778 2007 1996 2149 1010 4352 6811 5350 2081 1999 5386 2007 5611 9786 2489 3229 2000 2137 6089 1012 1996 8778 1010 2772 1999 11096 1010 2097 5323 2054 2024 2170 1000 4591 3919 10019 1000 1999 5279 1012 3688 2013 2122 10019 2097 5959 4611 2489 3229 2000 1996 2149 1010 3024 2008 3486 1003 1997 2037 6177 2024 1996 4031 1997 5611 1011 6811 6792 1012 1996 2149 5577 2023 2004 1996 2087 2590 3171 3820 2090 5279 1998 3956 1999 2048 5109 1012 1996 8778 7411 1996 10019 2038 2042 20659 2005 2086 1012 2045 2038 2042 2784 14639 1999 5279 2055 2151 2433 1997 2522 1011 3169 2007 3956 2004 2146 2004 2049 3521 2832 2007 1996 21524 3464 8534 1012 2021 1999 3522 3134 2019 5866 8251 2038 13147 2046 4262 2090 1996 2048 3032 1012 2119 10573 5895 3041 2023 3204 1010 2007 5279 13041 2067 2019 5611 2040 2038 2366 2809 2086 1999 3827 2044 2108 7979 2005 22624 1012 6811 2343 7570 2015 3490 14163 20709 2243 2038 2649 5611 3539 2704 16126 10666 2004 1996 2190 3382 2005 1996 21524 2000 6162 3521 1012 1996 2231 1999 11096 2085 7164 2720 10666 2003 3048 2875 1996 2803 1998 2185 2013 1996 4460 1997 2157 3358 2967 1012 2009 2036 7164 1996 2149 1010 4508 2011 2885 1010 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] cai ##rn shares up on new oil find shares in cai ##rn energy have jumped 6 % after the firm said an indian oil ##field was larger than previously thought . cai ##rn said drilling to the north - west of its development site in rajasthan had produced \" very strong results \" . the company also said it now believed the development area would be able to produce oil for more than 25 years . cai ##rn ' s share price rose 300 % last year after a number of oil finds , but its shares were hit in december following a disappointing drilling update . december ' s share fall means that cai ##rn is still in danger of being relegated from the ft ##se 100 when the index is res ##hu ##ffled next month . cai ##rn ' s shares closed up 64 pen ##ce , or 6 % , at 113 ##0 ##p on thursday . before christmas , cai ##rn revealed that drilling to the north of the field in rajasthan had been disappointing , which caused its shares to lose 18 % in one day . however , on thursday , the group said its belief that the path of oil in the area actually moved further to the west had proved correct . \" this area does need more app ##rai ##sal drilling but it looks very strong , \" dr mike watts head of exploration said . chief executive bill ga ##mmel ##l added : \" [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] cai ##rn shares up on new oil find shares in cai ##rn energy have jumped 6 % after the firm said an indian oil ##field was larger than previously thought . cai ##rn said drilling to the north - west of its development site in rajasthan had produced \" very strong results \" . the company also said it now believed the development area would be able to produce oil for more than 25 years . cai ##rn ' s share price rose 300 % last year after a number of oil finds , but its shares were hit in december following a disappointing drilling update . december ' s share fall means that cai ##rn is still in danger of being relegated from the ft ##se 100 when the index is res ##hu ##ffled next month . cai ##rn ' s shares closed up 64 pen ##ce , or 6 % , at 113 ##0 ##p on thursday . before christmas , cai ##rn revealed that drilling to the north of the field in rajasthan had been disappointing , which caused its shares to lose 18 % in one day . however , on thursday , the group said its belief that the path of oil in the area actually moved further to the west had proved correct . \" this area does need more app ##rai ##sal drilling but it looks very strong , \" dr mike watts head of exploration said . chief executive bill ga ##mmel ##l added : \" [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 29080 6826 6661 2039 2006 2047 3514 2424 6661 1999 29080 6826 2943 2031 5598 1020 1003 2044 1996 3813 2056 2019 2796 3514 3790 2001 3469 2084 3130 2245 1012 29080 6826 2056 15827 2000 1996 2167 1011 2225 1997 2049 2458 2609 1999 16815 2018 2550 1000 2200 2844 3463 1000 1012 1996 2194 2036 2056 2009 2085 3373 1996 2458 2181 2052 2022 2583 2000 3965 3514 2005 2062 2084 2423 2086 1012 29080 6826 1005 1055 3745 3976 3123 3998 1003 2197 2095 2044 1037 2193 1997 3514 4858 1010 2021 2049 6661 2020 2718 1999 2285 2206 1037 15640 15827 10651 1012 2285 1005 1055 3745 2991 2965 2008 29080 6826 2003 2145 1999 5473 1997 2108 7049 2013 1996 3027 3366 2531 2043 1996 5950 2003 24501 6979 28579 2279 3204 1012 29080 6826 1005 1055 6661 2701 2039 4185 7279 3401 1010 2030 1020 1003 1010 2012 12104 2692 2361 2006 9432 1012 2077 4234 1010 29080 6826 3936 2008 15827 2000 1996 2167 1997 1996 2492 1999 16815 2018 2042 15640 1010 2029 3303 2049 6661 2000 4558 2324 1003 1999 2028 2154 1012 2174 1010 2006 9432 1010 1996 2177 2056 2049 6772 2008 1996 4130 1997 3514 1999 1996 2181 2941 2333 2582 2000 1996 2225 2018 4928 6149 1012 1000 2023 2181 2515 2342 2062 10439 14995 12002 15827 2021 2009 3504 2200 2844 1010 1000 2852 3505 11042 2132 1997 8993 2056 1012 2708 3237 3021 11721 29033 2140 2794 1024 1000 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 29080 6826 6661 2039 2006 2047 3514 2424 6661 1999 29080 6826 2943 2031 5598 1020 1003 2044 1996 3813 2056 2019 2796 3514 3790 2001 3469 2084 3130 2245 1012 29080 6826 2056 15827 2000 1996 2167 1011 2225 1997 2049 2458 2609 1999 16815 2018 2550 1000 2200 2844 3463 1000 1012 1996 2194 2036 2056 2009 2085 3373 1996 2458 2181 2052 2022 2583 2000 3965 3514 2005 2062 2084 2423 2086 1012 29080 6826 1005 1055 3745 3976 3123 3998 1003 2197 2095 2044 1037 2193 1997 3514 4858 1010 2021 2049 6661 2020 2718 1999 2285 2206 1037 15640 15827 10651 1012 2285 1005 1055 3745 2991 2965 2008 29080 6826 2003 2145 1999 5473 1997 2108 7049 2013 1996 3027 3366 2531 2043 1996 5950 2003 24501 6979 28579 2279 3204 1012 29080 6826 1005 1055 6661 2701 2039 4185 7279 3401 1010 2030 1020 1003 1010 2012 12104 2692 2361 2006 9432 1012 2077 4234 1010 29080 6826 3936 2008 15827 2000 1996 2167 1997 1996 2492 1999 16815 2018 2042 15640 1010 2029 3303 2049 6661 2000 4558 2324 1003 1999 2028 2154 1012 2174 1010 2006 9432 1010 1996 2177 2056 2049 6772 2008 1996 4130 1997 3514 1999 1996 2181 2941 2333 2582 2000 1996 2225 2018 4928 6149 1012 1000 2023 2181 2515 2342 2062 10439 14995 12002 15827 2021 2009 3504 2200 2844 1010 1000 2852 3505 11042 2132 1997 8993 2056 1012 2708 3237 3021 11721 29033 2140 2794 1024 1000 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] saudi nc ##ci ' s shares so ##ar shares in saudi arabia ' s national company for cooperative insurance ( nc ##ci ) soared on their first day of trading in ri ##yad ##h . they were trading 84 % above the offer price on monday , changing hands at 37 ##2 ri ##yal ##s ( $ 99 ; a ## ##53 ) after topping 400 early in the day . demand for the ins ##urer ' s debut shares was strong - 12 times what was on sale . the listing was part of the country ' s plans to open up its insurance market and boost demand in the sector . der ##eg ##ulation is expected to boost demand for accident and damage cover . previously , only nc ##ci has been legally allowed to offer insurance products within saudi arabia . however , the authorities have turned a blind eye to the many other firms selling insurance . saudi arabia now wants a fully functioning insurance industry and is introducing legislation that will cl ##amp down on una ##uth ##oris ##ed companies . policy - makers also want to make having insurance more of a requirement , but first have to take steps to boost public confidence in the system , analysts said . as a result , nc ##ci is being developed as the industry ' s flagship firm - publicly - listed , with audit ##ed accounts . saudi arabia sold 7 million nc ##ci shares , or about [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] saudi nc ##ci ' s shares so ##ar shares in saudi arabia ' s national company for cooperative insurance ( nc ##ci ) soared on their first day of trading in ri ##yad ##h . they were trading 84 % above the offer price on monday , changing hands at 37 ##2 ri ##yal ##s ( $ 99 ; a ## ##53 ) after topping 400 early in the day . demand for the ins ##urer ' s debut shares was strong - 12 times what was on sale . the listing was part of the country ' s plans to open up its insurance market and boost demand in the sector . der ##eg ##ulation is expected to boost demand for accident and damage cover . previously , only nc ##ci has been legally allowed to offer insurance products within saudi arabia . however , the authorities have turned a blind eye to the many other firms selling insurance . saudi arabia now wants a fully functioning insurance industry and is introducing legislation that will cl ##amp down on una ##uth ##oris ##ed companies . policy - makers also want to make having insurance more of a requirement , but first have to take steps to boost public confidence in the system , analysts said . as a result , nc ##ci is being developed as the industry ' s flagship firm - publicly - listed , with audit ##ed accounts . saudi arabia sold 7 million nc ##ci shares , or about [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 8174 13316 6895 1005 1055 6661 2061 2906 6661 1999 8174 9264 1005 1055 2120 2194 2005 10791 5427 1006 13316 6895 1007 29127 2006 2037 2034 2154 1997 6202 1999 15544 25152 2232 1012 2027 2020 6202 6391 1003 2682 1996 3749 3976 2006 6928 1010 5278 2398 2012 4261 2475 15544 21095 2015 1006 1002 5585 1025 1037 29646 22275 1007 2044 22286 4278 2220 1999 1996 2154 1012 5157 2005 1996 16021 27595 1005 1055 2834 6661 2001 2844 1011 2260 2335 2054 2001 2006 5096 1012 1996 10328 2001 2112 1997 1996 2406 1005 1055 3488 2000 2330 2039 2049 5427 3006 1998 12992 5157 1999 1996 4753 1012 4315 13910 9513 2003 3517 2000 12992 5157 2005 4926 1998 4053 3104 1012 3130 1010 2069 13316 6895 2038 2042 10142 3039 2000 3749 5427 3688 2306 8174 9264 1012 2174 1010 1996 4614 2031 2357 1037 6397 3239 2000 1996 2116 2060 9786 4855 5427 1012 8174 9264 2085 4122 1037 3929 12285 5427 3068 1998 2003 10449 6094 2008 2097 18856 16613 2091 2006 14477 14317 21239 2098 3316 1012 3343 1011 11153 2036 2215 2000 2191 2383 5427 2062 1997 1037 9095 1010 2021 2034 2031 2000 2202 4084 2000 12992 2270 7023 1999 1996 2291 1010 18288 2056 1012 2004 1037 2765 1010 13316 6895 2003 2108 2764 2004 1996 3068 1005 1055 10565 3813 1011 7271 1011 3205 1010 2007 15727 2098 6115 1012 8174 9264 2853 1021 2454 13316 6895 6661 1010 2030 2055 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 8174 13316 6895 1005 1055 6661 2061 2906 6661 1999 8174 9264 1005 1055 2120 2194 2005 10791 5427 1006 13316 6895 1007 29127 2006 2037 2034 2154 1997 6202 1999 15544 25152 2232 1012 2027 2020 6202 6391 1003 2682 1996 3749 3976 2006 6928 1010 5278 2398 2012 4261 2475 15544 21095 2015 1006 1002 5585 1025 1037 29646 22275 1007 2044 22286 4278 2220 1999 1996 2154 1012 5157 2005 1996 16021 27595 1005 1055 2834 6661 2001 2844 1011 2260 2335 2054 2001 2006 5096 1012 1996 10328 2001 2112 1997 1996 2406 1005 1055 3488 2000 2330 2039 2049 5427 3006 1998 12992 5157 1999 1996 4753 1012 4315 13910 9513 2003 3517 2000 12992 5157 2005 4926 1998 4053 3104 1012 3130 1010 2069 13316 6895 2038 2042 10142 3039 2000 3749 5427 3688 2306 8174 9264 1012 2174 1010 1996 4614 2031 2357 1037 6397 3239 2000 1996 2116 2060 9786 4855 5427 1012 8174 9264 2085 4122 1037 3929 12285 5427 3068 1998 2003 10449 6094 2008 2097 18856 16613 2091 2006 14477 14317 21239 2098 3316 1012 3343 1011 11153 2036 2215 2000 2191 2383 5427 2062 1997 1037 9095 1010 2021 2034 2031 2000 2202 4084 2000 12992 2270 7023 1999 1996 2291 1010 18288 2056 1012 2004 1037 2765 1010 13316 6895 2003 2108 2764 2004 1996 3068 1005 1055 10565 3813 1011 7271 1011 3205 1010 2007 15727 2098 6115 1012 8174 9264 2853 1021 2454 13316 6895 6661 1010 2030 2055 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 256 tokens long.\n",
    "MAX_SEQ_LENGTH = 256\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4fyq4sWKZ_Bd",
    "outputId": "c119b369-320d-4bf3-c725-089ba4403785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GWcrXRTCaGCl",
    "outputId": "757fe023-6f4c-445e-d403-02ed2d74e5bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bert.run_classifier.InputFeatures object at 0x7f8725ec9048>\n"
     ]
    }
   ],
   "source": [
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction.\n",
    "\n",
    "Note: Turn off the metrics for the multi-class cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        # f1_score = tf.contrib.metrics.f1_score(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)\n",
    "        # auc = tf.metrics.auc(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)\n",
    "        # recall = tf.metrics.recall(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)\n",
    "        # precision = tf.metrics.precision(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels) \n",
    "        # true_pos = tf.metrics.true_positives(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)\n",
    "        # true_neg = tf.metrics.true_negatives(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)   \n",
    "        # false_pos = tf.metrics.false_positives(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)  \n",
    "        # false_neg = tf.metrics.false_negatives(\n",
    "        #     label_ids,\n",
    "        #     predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy##,\n",
    "            # \"f1_score\": f1_score,\n",
    "            # \"auc\": auc,\n",
    "            # \"precision\": precision,\n",
    "            # \"recall\": recall,\n",
    "            # \"true_positives\": true_pos,\n",
    "            # \"true_negatives\": true_neg,\n",
    "            # \"false_positives\": false_pos,\n",
    "            # \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "q_WebpS1X97v",
    "outputId": "5fdd534f-4552-4b66-b44b-39d88bfad09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f86bf5540b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f86bf5540b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "aa8a9d4e-7429-49a0-9285-9d3f8ff4a8e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.6222804, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.6222804, step = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.541533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.541533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.07749912, step = 100 (184.662 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.07749912, step = 100 (184.662 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 177 into OUTPUT_DIR_NAME/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 177 into OUTPUT_DIR_NAME/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.0049373796.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.0049373796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:06:02.958145\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEeEUIp0hFPH"
   },
   "source": [
    "##### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "E_WU2EpHg5HI",
    "outputId": "893a62ba-6428-44d3-9268-64c71ce13456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-06-10T23:16:09Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-06-10T23:16:09Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-06-10-23:16:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-06-10-23:16:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 177: eval_accuracy = 0.9989424, global_step = 177, loss = 0.0074916948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 177: eval_accuracy = 0.9989424, global_step = 177, loss = 0.0074916948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 177: OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 177: OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.9989424, 'global_step': 177, 'loss': 0.0074916948}"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "eval_train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "estimator.evaluate(input_fn=eval_train_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMLEUCHChSC8"
   },
   "source": [
    "##### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "51036efe-d2e6-491b-a3c6-3ad1d73200cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-06-10T23:11:21Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-06-10T23:11:21Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-06-10-23:11:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-06-10-23:11:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 177: eval_accuracy = 0.98203593, global_step = 177, loss = 0.039279792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 177: eval_accuracy = 0.98203593, global_step = 177, loss = 0.039279792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 177: OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 177: OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.98203593, 'global_step': 177, 'loss': 0.039279792}"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpbnakNpggzN"
   },
   "source": [
    "Training result:  \n",
    "``{'eval_accuracy': 0.9989424, 'global_step': 177, 'loss': 0.0074916948}``\n",
    "\n",
    "Testing result:  \n",
    "``{'eval_accuracy': 0.98203593, 'global_step': 177, 'loss': 0.039279792}``\n",
    "\n",
    "\n",
    "Visualize the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "I4M5NNq6iGMC",
    "outputId": "1a8891f0-5421-43b8-8049-b51b76cbbdea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "test_pred_generator = estimator.predict(input_fn=test_input_fn)\n",
    "test_pred_result = list(test_pred_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "NhFtH_zyjO0S",
    "outputId": "540afd05-23ae-4aa9-8971-7bae78a246f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': 3,\n",
       "  'probabilities': array([-7.1887426e+00, -6.5005150e+00, -6.7207894e+00, -4.4607422e-03,\n",
       "         -6.9203591e+00], dtype=float32)},\n",
       " {'labels': 2,\n",
       "  'probabilities': array([-5.032318 , -6.8917365, -0.0098919, -6.638263 , -6.9137273],\n",
       "        dtype=float32)},\n",
       " {'labels': 0,\n",
       "  'probabilities': array([-4.1497555e-03, -7.1691031e+00, -6.3769398e+00, -7.2736192e+00,\n",
       "         -6.9307895e+00], dtype=float32)},\n",
       " {'labels': 0,\n",
       "  'probabilities': array([-5.4481360e-03, -6.8471060e+00, -6.4099474e+00, -6.6095715e+00,\n",
       "         -6.5869465e+00], dtype=float32)},\n",
       " {'labels': 0,\n",
       "  'probabilities': array([-3.7806004e-03, -6.9856677e+00, -6.9201832e+00, -6.9427691e+00,\n",
       "         -7.0185180e+00], dtype=float32)}]"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YOFCSuOgiVep",
    "outputId": "7fc43756-0006-4602-96ad-35bb7507f699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: sport, True Label: sport \n",
      "\n",
      " Ireland call up uncapped Campbell\n",
      "\n",
      "Ulster scrum-half Kieran Campbell is one of five uncapped players included in Ireland's RBS Six Nations squad.\n",
      "\n",
      "Campbell is joined by Ulster colleagues Roger Wilson and Ronan McCormack along with Connacht's Bernard Jackman and Munster's Shaun Payne. Gordon D'Arcy is back after injury while Munster flanker Alan Quinlan also returns to international consideration. \"The squad is selected purely on form. A lot of players put their hands up,\" coach Eddie O'Sullivan told BBC Sport. \"Kieran Campbell was just one of those players. He has been playing very well in the Heineken Cup and deserves his call-up. \"There is big competition in some departments and not so much in others. There were one or two players who were unfortunate just to miss out.\" Back-row forwards David Wallace and Victor Costello are omitted, with O'Sullivan having Quinlan, Wilson, Simon Easterby, Anthony Foley, Denis Leamy and Johnny O'Connor vying for the three positions.\n",
      "\n",
      "With David Humphreys, Kevin Maggs, Simon Best and Tommy Bowe again included, it is Ulster's biggest representation in a training panel for quite some time. Munster and Leinster have 12 and 11 players in the squad respectively while Jackman is the sole Connacht representative. Four British-based players are also included. Ulster forward Ronan McCormack said he was \"totally shocked\" to be included. \"I'm really looking forward to it,\" said McCormack. \"I played with guys like Brian O'Driscoll and Denis Hickie back in my school days in Leinster so I do know a few of them although not that well. \"It will be great to work with them.\"\n",
      "\n",
      "S Best (Ulster), S Byrne (Leinster), R Corrigan (Leinster), L Cullen (Leinster), S Easterby (Llanelli), A Foley (Munster), J Hayes (Munster), M Horan (Munster), B Jackman (Connacht), D Leamy (Munster), E Miller (Leinster), R McCormack (Ulster), D O'Callaghan (Munster), P O'Connell (Munster), J O'Connor (Wasps), M O'Kelly (Leinster), F Sheahan (Munster), R Wilson (Ulster), A Quinlan (Munster).\n",
      "\n",
      "T Bowe (Ulster), K Campbell (Ulster), G D'Arcy (Ulster), G Dempsey (Leinster), G Duffy (Harlequins), G Easterby (Leinster), D Hickie (Leinster), A Horgan (Munster), S Horgan (Leinster), D Humphreys (Ulster), K Maggs (Ulster), G Murphy (Leicester), B O'Driscoll, (Leinster), R O'Gara (Munster), S Payne (Munster), P Stringer (Munster).\n",
      "\n",
      "K Gleeson (Leinster), T Howe (Ulster), J Kelly (Munster), N McMillan (Ulster).\n",
      "###############\n",
      "\n",
      "Prediction: politics, True Label: politics \n",
      "\n",
      " Gurkhas to help tsunami victims\n",
      "\n",
      "Britain has offered to send a company of 120 Gurkhas to assist with the tsunami relief effort in Indonesia, Downing Street said.\n",
      "\n",
      "The deployment would involve troops from the 2nd Battalion Royal Gurkha Rifles, based in Brunei. Discussions have begun with Indonesia on the exact timing and location of the deployment, but the government said the offer was aimed at the Aceh province. Downing St said a similar offer might be made to the Sri Lankan government.\n",
      "\n",
      "However a spokesman pointed out that there were particular logistical difficulties in Indonesia which the Gurkhas might be able to help with. The spokesman said: \"Following this morning's daily coordination meeting on the post-tsunami relief effort, the government has formally offered the Indonesian government the assistance of a company of British Army Gurkhas from 2nd Battalion Royal Gurkha Rifles around 120 personnel and two helicopters. \"This is in addition to the ships and aircraft we have already committed to the relief operation in the Indian Ocean.\"\n",
      "\n",
      "Indonesia was by far the country worst affected by the tsunami, with 94,000 of the 140,000 confirmed deaths so far. International Development Minister Gareth Thomas said the assistance offer would most likely focus on the northern province of Aceh. \"We have offered the Gurkhas to help in the process of scaling up the relief effort, particularly in Aceh which is undoubtedly the hardest hit area in the Indian Ocean at the moment,\" he said. \"We've also had RAF aircraft flying in equipment which the UN desperately need in order to set up a truly effective relief operation on the ground in Aceh province as well.\" The offer comes as the Foreign Secretary Jack Straw arrives in Indonesia for a special summit meeting on the disaster.\n",
      "###############\n",
      "\n",
      "Prediction: business, True Label: business \n",
      "\n",
      " Egypt and Israel seal trade deal\n",
      "\n",
      "In a sign of a thaw in relations between Egypt and Israel, the two countries have signed a trade protocol with the US, allowing Egyptian goods made in partnership with Israeli firms free access to American markets.\n",
      "\n",
      "The protocol, signed in Cairo, will establish what are called \"qualified industrial zones\" in Egypt. Products from these zones will enjoy duty free access to the US, provided that 35% of their components are the product of Israeli-Egyptian cooperation. The US describes this as the most important economic agreement between Egypt and Israel in two decades.\n",
      "\n",
      "The protocol establishing the zones has been stalled for years. There has been deep sensitivity in Egypt about any form of co-operation with Israel as long as its peace process with the Palestinians remains blocked. But in recent weeks an unusual warmth has crept into relations between the two countries. Both exchanged prisoners earlier this month, with Egypt handing back an Israeli who has served eight years in prison after being convicted for spying.\n",
      "\n",
      "Egyptian President Hosni Mubarak has described Israeli Prime Minister Ariel Sharon as the best chance for the Palestinians to achieve peace. The government in Cairo now believes Mr Sharon is moving towards the centre and away from the positions of right wing groups. It also believes the US, pressed by Europe, is now more willing to engage seriously in the search for a settlement. But there are also pressing economic reasons for Egypt's decision to enter into the trade agreement. It will give a huge boost to Egyptian textile exports, which are about to suffer a drop after new regulations come into force in the US at the beginning of the year.\n",
      "###############\n",
      "\n",
      "Prediction: business, True Label: business \n",
      "\n",
      " Cairn shares up on new oil find\n",
      "\n",
      "Shares in Cairn Energy have jumped 6% after the firm said an Indian oilfield was larger than previously thought.\n",
      "\n",
      "Cairn said drilling to the north-west of its development site in Rajasthan had produced \"very strong results\". The company also said it now believed the development area would be able to produce oil for more than 25 years. Cairn's share price rose 300% last year after a number of oil finds, but its shares were hit in December following a disappointing drilling update. December's share fall means that Cairn is still in danger of being relegated from the FTSE 100 when the index is reshuffled next month. Cairn's shares closed up 64 pence, or 6%, at 1130p on Thursday.\n",
      "\n",
      "Before Christmas, Cairn revealed that drilling to the north of the field in Rajasthan had been disappointing, which caused its shares to lose 18% in one day.\n",
      "\n",
      "However, on Thursday, the group said its belief that the path of oil in the area actually moved further to the west had proved correct. \"This area does need more appraisal drilling but it looks very strong,\" Dr Mike Watts head of exploration said. Chief executive Bill Gammell added: \"The more we progress in Rajasthan the better we feel about it.\" Cairn made the discovery after having been granted an extension to their drilling licence in January by Indian authorities. The firm has applied for a 30-month extension to scout for oil outside its main development area, which includes the Mangala and Aishwariya fields where Cairn has previously announced major discoveries. It also said production at its other fields across the globe was likely to surpass levels seen in 2004.\n",
      "###############\n",
      "\n",
      "Prediction: business, True Label: business \n",
      "\n",
      " Saudi NCCI's shares soar\n",
      "\n",
      "Shares in Saudi Arabia's National Company for Cooperative Insurance (NCCI) soared on their first day of trading in Riyadh.\n",
      "\n",
      "They were trading 84% above the offer price on Monday, changing hands at 372 riyals ($99; 53) after topping 400 early in the day. Demand for the insurer's debut shares was strong - 12 times what was on sale. The listing was part of the country's plans to open up its insurance market and boost demand in the sector. Deregulation is expected to boost demand for accident and damage cover.\n",
      "\n",
      "Previously, only NCCI has been legally allowed to offer insurance products within Saudi Arabia. However, the authorities have turned a blind eye to the many other firms selling insurance. Saudi Arabia now wants a fully functioning insurance industry and is introducing legislation that will clamp down on unauthorised companies. Policy-makers also want to make having insurance more of a requirement, but first have to take steps to boost public confidence in the system, analysts said. As a result, NCCI is being developed as the industry's flagship firm - publicly-listed, with audited accounts. Saudi Arabia sold 7 million NCCI shares, or about 70% of the company's total capital last month. More than 800,000 applicants got 9 shares each for 205 riyals apiece.\n",
      "###############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# category_codes = {\n",
    "#     'business': 0,\n",
    "#     'entertainment': 1,\n",
    "#     'politics': 2,\n",
    "#     'sport': 3,\n",
    "#     'tech': 4\n",
    "# }\n",
    "\n",
    "code_2_category = dict([(v, k) for k, v in category_codes.items()])\n",
    "\n",
    "cnt = 0\n",
    "for text, label, res in zip(test['Content'], test['Category'], test_pred_result):\n",
    "  print('Prediction: {}, True Label: {} \\n\\n {}'.format(code_2_category[res['labels']], label, text))\n",
    "  print('###'*5 + '\\n')\n",
    "  cnt += 1\n",
    "  if cnt == 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s8bbdXfSnZM0",
    "outputId": "9f6cc438-fe8b-4762-8a9b-9c8c17658760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: business, True Label: politics \n",
      "\n",
      " How political squabbles snowball\n",
      "\n",
      "It's become commonplace to argue that Blair and Brown are like squabbling school kids and that they (and their supporters) need to grow up and stop bickering.\n",
      "\n",
      "But this analysis in fact gets it wrong. It's not just children who fight - adults do too. And there are solid reasons why even a trivial argument between mature protagonists can be hard to stop once its got going. The key feature of an endless feud is that everyone can agree they'd be better off if it ended - but everyone wants to have the last word.\n",
      "\n",
      "Each participant genuinely wants the row to stop, but thinks it worth prolonging the argument just a tiny bit to ensure their view is heard. Their successive attempts to end the argument with their last word ensure the argument goes on and on and on. (In the case of Mr Blair and Mr Brown, successive books are published, ensuring the issues never die.) Now this isn't because the participants are stupid - it's actually each individual behaving entirely rationally, given the incentives facing them. Indeed, there's even a piece of economic theory that explains all this. Nothing as obscure as \"post-neo-classical endogenous growth theory\" which the chancellor himself once quoted - but a ubiquitous piece of game theory which all respectable policy wonks are familiar with.\n",
      "\n",
      "It's often referred to as the \"prisoner's dilemma\", based on a parable much told in economics degree courses... about a sheriff and two prisoners. The story goes that two prisoners are jointly charged with a heinous crime, and are locked up in separate cells. But the sheriff desperately needs a confession from at least one of them, to provide enough evidence to convict them of the crime. Without a confession, the prisoners will get a minimal sentence on some trumped up charge.\n",
      "\n",
      "Clearly the prisoners' best strategy is to keep their mouths shut, and take the short sentence, but the clever sheriff has an idea to induce them to talk. He tells each prisoner separately, that if they confess - and they are the only one to confess - they'll be let off their crime. And he tells them that if they don't confess - and they are the only one not to confess - they'll get life. Now, if you are prisoner confronted with this choice, your best bet is to confess. If your partner doesn't confess, you'll get off completely. And if your partner does confess, you'd better confess to ensure you don't get life. The result is of course, both prisoners confess, so the sheriff does not have to let either one off. Both prisoners' individual logic was to behave that way, even though both would have been better if they had somehow agreed to shut up. Don't worry if you don't entirely follow it - you can to look it up on Google, where there are 283,000 entries on it.\n",
      "\n",
      "The prisoners' dilemma and all its ramifications have truly captured economists in the last couple of decades. It is a parable used to describe any situation where there is an obvious sensible choice to be taken collectively, but where the only rational choice individually is to behave selfishly.\n",
      "\n",
      "A cold war arms race for example - a classic case where both Russia and America would be better off with just a few arms, rather than a lot of arms. But as long as each wants just a few more arms than the other, an arms race ensues with the results that the individually logical decision to buy more arms, results in arms levels that are too high. What economics tells us is that once you're in a prisoners' dilemma - unless you are repeating the experience many times over - it's hard to escape the perverse logic of it. It's no good just exhorting people to stop buying arms, or to stop arguing when all their incentives encourage them to carry on. Somehow, the incentives have to change.\n",
      "\n",
      "In the case of the Labour Party, if you believe the rift between Blair and Brown camps is as bad as the reports suggest, Solomon's wisdom needs to be deployed to solve the problem. Every parent knows there are ingenious solutions to arguments, solutions which affect the incentives of the participants. An example, is the famous rule that \"one divides, the other chooses\" as a way of allocating a piece of cake to be sliced up between greedy children. In the case of an apparently endless argument, if you want it to come to an end, you have to ensure the person who has the last word is one who loses rather than the one who wins the row. The cost of prolonging the row by even one more briefing, or one more book for that matter, has to exceed the benefit of having the last word, and getting your point in. If the rest of the party can enforce that, they'll have the protagonists retreating pretty quickly.\n",
      "###############\n",
      "\n",
      "Prediction: business, True Label: tech \n",
      "\n",
      " US blogger fired by her airline\n",
      "\n",
      "A US airline attendant suspended over \"inappropriate images\" on her blog - web diary - says she has been fired.\n",
      "\n",
      "Ellen Simonetti, known as Queen of the Sky, wrote an anonymous semi-fictional account of her life in the sky. She was suspended by Delta in September. In a statement, she said she was initiating legal action against the airline for \"wrongful termination\". A Delta spokesperson confirmed on Wednesday that Ms Simonetti was no longer an employee. Delta has repeatedly declined to elaborate on what it calls \"internal employee matters\". A spokesperson reiterated this position on Wednesday, confirming only that Ms Simonetti was no longer with the company. The spokesperson also confirmed that there were \"very clear rules\" attached to the unauthorised use of Delta branding, including uniforms. Ms Simonetti announced on her blog she had been fired on 1 November.\n",
      "\n",
      "She said in an official statement: \"As a result of my suspension and subsequent termination without cause by Delta Airlines I am moving forward with filing a discrimination complaint with the Federal Government EEOC [US Equal Employment Opportunity Commission].\" She added she had also hired a Texas-based law firm to initiate legal action for \"wrongful termination, defamation of character and lost future wages.\" Ms Simonetti told the BBC News website she had received no warning or further explanation when she was suspended on 25 September. Queen of the Sky has received a lot of support and advice from the global blogging community since news of her suspension was brought to light on the BBC News website and others.\n",
      "\n",
      "Her story has highlighted concerns amongst the growing blogging community about conflicts of interest, employment law and free speech on personal websites. The blog, which she started in January as a way of getting over her mother's death, contains a mix of fictional and non-fictional accounts. Queen of the Sky developed over the months as a character in her own right, according to Ms Simonetti. In the postings, she made up fictional names for cities and other companies she mentioned to protect anonymity. But some postings contained images of herself in uniform. Of the 10 or so images only one showed Ms Simonetti's flight \"wings\". She removed them as soon as she was informed of her suspension. \"I never meant it as something to harm my company and don't understand how they think it did harm them,\" Ms Simonetti said. A legal expert in the US speculated that Delta might be concerned that the fictional content on the blog may be linked back to the airline after the images were posted.\n",
      "\n",
      "Delta has been hit recently by pressures of rising fuel costs and fierce competition. It has said it needs to cut between 6,000 and 7,000 jobs and reduce costs by $5bn (2.7bn) a year. Analysts had warned recently that the airline might have to seek Chapter 11 bankruptcy prevention. Last week, it struck a $1bn cost-cutting deal with its pilots which could save it from bankruptcy. The deal would see pilots accept a 32% pay cut in return for the right to buy 30 million Delta shares, unions said. And on Monday, it negotiated a deal to defer about $135m in debt which was due next year, until 2007. The airline also said it had agreed the terms of a $600m loan from American Express.\n",
      "###############\n",
      "\n",
      "Prediction: business, True Label: politics \n",
      "\n",
      " BAA support ahead of court battle\n",
      "\n",
      "UK airport operator BAA has reiterated its support for the government's aviation expansion plans to airports throughout the country.\n",
      "\n",
      "The comments come a day ahead of a High Court challenge by residents' groups and local councils to the government's White Paper. The judicial review will centre on government plans for expansion at Heathrow, Stansted and Luton airports. BAA, which operates all three, said it was consulting with local communities. \"We are...consulting on voluntary compensation schemes which go beyond our statutory obligations,\" a BAA spokesman said.\n",
      "\n",
      "Groups challenging the plans include Stop Stansted Expansion, Heathrow anti-noise campaigners HACAN Clearskies and the London boroughs of Hillingdon and Wandsworth. At Heathrow, Gatwick, Edinburgh and Glasgow airports, BAA launched a series of consultations on blight to properties from the proposed expansion in September 2004, which will close next week. The company is also offering to buy noise-hit properties for an index-linked, unblighted price. Among other measures, BAA has set up a homeowner support scheme for people living near Stansted, and has launched a special scheme for those close to the airport but far enough away not to be covered by the homeowner scheme. At Heathrow, BAA said it was working closely with all interested parties to see how the strict environmental, air quality and noise targets for a third runway can be met.\n",
      "\n",
      "At Gatwick, the company has written to homes and business likely to be affected by any extra runway. Stop Stansted Expansion said the White Paper, published in December 2003, was \"fundamentally flawed\" and did not follow the proper consultation process. \"We do not underestimate the scale of the challenge before us because the courts have never before overturned a government White Paper,\" said Stop Stansted Expansion chairman Peter Sanders said. HACAN chairman John Stewart said: \"Almost exactly a year ago the government published its 30-year aviation White Paper with much fanfare. \"It hoped that would be the end of the debate and it could proceed with its plans for a massive expansion of aviation. \"Yet, a year later the protesters are still here, and stronger than ever. \" A judgement from Mr Justice Sullivan is expected early in February.\n",
      "###############\n",
      "\n",
      "Prediction: business, True Label: sport \n",
      "\n",
      " Bates seals takeover\n",
      "\n",
      "Ken Bates has completed his takeover of Leeds United.\n",
      "\n",
      "The 73-year-old former Chelsea chairman sealed the deal at 0227 GMT on Friday, and has bought a 50% stake in the club. He said: \"I'm delighted to be stepping up to the mantel at such a fantastic club. I recognise Leeds as a great club that has fallen on hard times. \"We have a lot of hard work ahead to get the club back where it belongs in the Premiership, and with the help of our fans we will do everything we can.\" Bates bought his stake under the guise of a Geneva-based company known as The Forward Sports Fund. He revealed that part of his plan is to buy back Leeds' Elland Road stadium and Thorp Arch training ground in due course.\n",
      "\n",
      "\"It's going to be a tough jon and the first task is to stabilise the cash flow and sort out the remaining creditors,\" Bates added. \"But there is light at the end of a very long tunnel. For the past year it has been a matter of firefighting - now we can start running the club again.\" Outgoing Leeds chairman Gerald Krasner said: \"This deal ensures the medium to long term survival of the club and I believe Mr Bates' proposals are totally for the benefit of the club. \"We are content that under Mr Bates, Leeds United will continue to consolidate and move forward. \"When we took over Leeds United in March 2004, the club had a debt of 103m, since that date, my board has succeeded in reducing the debt to under 25m. \"We worked tirelessly to solve all of the problems at Leeds United. \"Eighty percent of the problems have already been overcome and we came to this agreement with Mr Bates to secure its ongoing success.\" Krasner revealed that his consortium has been asked to remain in the background at the club for an undisclosed period to help ensure a smooth hand-over. He will stay on in an unpaid capacity while Peter Lorimer will continue in his role as director and point of contact for the fans and Peter McCormick will serve as a consultant to the incoming board. The other outgoing directors have agreed to leave their loans of 4.5m in the company for the next four years. On Leeds' new-look board it is understood that Lorimer will be joined by former Chelsea finance director Yvonne Todd and Bates' lawyer Mark Taylor.\n",
      "\n",
      "Krasner refused to give any details of the finances involved in the takeover. He told BBC Five Live: \"I am not going into the figures. If Ken wants to give them up that is up to him. I can not tell you what the money will be used for. \"This dea l is not about money for the current board. In the last four months I never saw any cheques until this week from one person. I am not stretching figures, we don't discuss internal arrangements.\" Bates stepped down as Chelsea chairman in March last year following Roman Abramovich's 140m takeover at Stamford Bridge. In May, he made a proposal to invest 10m in Sheffield Wednesday, but this was rejected by the club. Sebastien Sainsbury had been close to a takeover of Leeds but withdrew his 25m offer last week.\n",
      "\n",
      "His efforts failed after he revealed it would take 40m to stage a takeover, and that the club will also lose 10m over the next six months. The club was on the brink of administration - and the deduction of 10 points by the Football League - before Bates' arrival but his investment has spared them that prospect.\n",
      "###############\n",
      "\n",
      "Prediction: entertainment, True Label: business \n",
      "\n",
      " McDonald's to sponsor MTV show\n",
      "\n",
      "McDonald's, the world's largest restaurant chain, is to sponsor a programme on music channel MTV as part of its latest youth market promotion.\n",
      "\n",
      "The show Advance Warning highlights new talent and MTV reckons it will give McDonald's access to nearly 400 million homes in 162 countries. McDonald's golden arches, name and \"I'm loving it\" catchphrase will be used throughout the half-hour programme. The move comes amid growing concerns about obesity in Europe and the US.\n",
      "\n",
      "The European Union has called on the food industry to reduce the number of adverts aimed at young children, warning that legislation would be introduced. unless voluntary steps were taken. In the US, food group Kraft is among firms that already have cut back on promoting sugar and fattening products to the young. McDonalds has also been taking steps to improve its junk food reputation, revamping its menu and providing clients with health-related products such as pedometers. As well as burgers like the Big Mac and Quarter Pounder with Cheese, the company now sells healthier options such as salads and fresh fruit. Chief executive Jim Skinner attributed an 8.3% increase in January worldwide sales to the \"vitality of our menu\", among other things.\n",
      "\n",
      "Hooking up with MTV is expected to add extra momentum to McDonald's recent revival. MTV, which played a key role in the emergence of the music video, is to show Advance Warning on all 25 of its channels across the world. The programme can at present only been seen in the US, where it has featured artists like British stars Joss Stone and Franz Ferdinand. McDonald's has targeted the youth market in the past with its advertisements, signing up stars like jelly-legged dancer Justin Timberlake and all-woman singing group Destiny's Child.\n",
      "###############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Focus on the wrong cases:\n",
    "cnt = 0\n",
    "for text, label, res in zip(test['Content'], test['Category'], test_pred_result):\n",
    "  pred_label = code_2_category[res['labels']]\n",
    "  if pred_label == label:\n",
    "    continue\n",
    "  print('Prediction: {}, True Label: {} \\n\\n {}'.format(pred_label, label, text))\n",
    "  print('###'*5 + '\\n')\n",
    "  cnt += 1\n",
    "  if cnt == 5:\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF1.15.3 BBC News Classification with BERT on TF Hub fzsea.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
